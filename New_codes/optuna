#Otama_iT_Kfold_Optuna 
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow INFO and WARNING logs
os.environ['CUDA_LOG_LEVEL'] = 'ERROR'    # Suppress CUDA warnings
os.environ['NV_LOG_LEVEL'] = 'ERROR'      # Suppress NVIDIA-specific logs

import logging
logging.getLogger('tensorflow').setLevel(logging.WARNING)  # Additional filter for TensorFlow

import tensorflow as tf
tf.debugging.set_log_device_placement(False)  # Disable device placement logs

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import random
import sys
from contextlib import contextmanager
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Add, Layer, GaussianNoise
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback
from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, ReLU, Dropout, Flatten
from tensorflow.keras.regularizers import l2
from sklearn.model_selection import KFold
import optuna

# Context manager to suppress TensorFlow output during specific computations
@contextmanager
def suppress_output():
    null_device = 'nul' if os.name == 'nt' else '/dev/null'
    with open(null_device, 'w') as null:
        stdout, stderr = sys.stdout, sys.stderr
        sys.stdout, sys.stderr = null, null
        try:
            yield
        finally:
            sys.stdout, sys.stderr = stdout, stderr

# Wave direction transformation
def transform_direction(theta):
    """Transforms wave direction (dpm) into a value between 0 and 1."""
    theta = np.array(theta) % 360
    psi = np.where(theta <= 180, 1 - theta / 180, (theta - 180) / 180)
    return psi

# Custom learning rate scheduler
class WarmUpCosineDecay(Callback):
    def __init__(self, initial_lr, warmup_epochs, total_epochs, steps_per_epoch):
        super(WarmUpCosineDecay, self).__init__()
        self.initial_lr = initial_lr
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.steps_per_epoch = steps_per_epoch
        self.global_step = 0
        self.history = {}

    def on_batch_end(self, batch, logs=None):
        self.global_step += 1
        current_epoch = self.global_step // self.steps_per_epoch

        if current_epoch < self.warmup_epochs:
            lr = self.initial_lr * (self.global_step / (self.warmup_epochs * self.steps_per_epoch))
        else:
            progress = (self.global_step - self.warmup_epochs * self.steps_per_epoch) / \
                       ((self.total_epochs - self.warmup_epochs) * self.steps_per_epoch)
            lr = self.initial_lr * 0.5 * (1.0 + np.cos(np.pi * progress))

        self.model.optimizer.learning_rate.assign(lr)
        self.history.setdefault('lr', []).append(lr)

def set_seed(seed=42):
    np.random.seed(seed)
    random.seed(seed)
    tf.random.set_seed(seed)

set_seed(42)

# 1. Data Preparation
def load_and_preprocess_data(filepath):
    data = pd.read_csv(filepath, parse_dates=['dates'])

    data['days_since_start'] = (data['dates'] - data['dates'].min()).dt.days
    data['month'] = data['dates'].dt.month
    data["Hs_ma_7"] = data['Hs'].rolling(7).mean()
    data["Hs_ma_30"] = data['Hs'].rolling(30).mean()
    data["Tp_ma_7"] = data['Tp'].rolling(7).mean()
    data["Tp_ma_30"] = data['Tp'].rolling(30).mean()
    data["Dir_ma_7"] = data['Dir'].rolling(7).mean()
    data["Dir_ma_30"] = data['Dir'].rolling(30).mean()
    data["Eng_ma_7"] = data['Eng'].rolling(7).mean()
    data["Eng_ma_30"] = data['Eng'].rolling(30).mean()
    data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)
    data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)

    data = data.dropna()

    data['Dir_transformed'] = transform_direction(data['Dir'])

    data = data.dropna()

    train = data[data['dates'].dt.year <= 2020]
    test = data[data['dates'].dt.year > 2020]

    features = ['Hs', 'Tp', 'Eng', 'month_cos', 'month_sin', 'Dir_transformed']
    target = 'shore'

    scaler = StandardScaler()
    X_train = scaler.fit_transform(train[features])
    X_test = scaler.transform(test[features])

    y_scaler = StandardScaler()
    y_train = y_scaler.fit_transform(train[target].values.reshape(-1, 1)).flatten()
    y_test = y_scaler.transform(test[target].values.reshape(-1, 1)).flatten()

    train_dates = train['dates'].values
    test_dates = test['dates'].values

    return X_train, X_test, y_train, y_test, train_dates, test_dates, scaler, y_scaler

# 2. Create Sequences
def create_sequences(X, y, dates, window_size=90):
    X_seq, y_seq, date_seq = [], [], []
    for i in range(len(X) - window_size):
        X_seq.append(X[i:i+window_size])
        y_seq.append(y[i+window_size])
        date_seq.append(dates[i+window_size])
    return np.array(X_seq), np.array(y_seq), np.array(date_seq)

# Custom Layer for Transpose
class TransposeLayer(Layer):
    def __init__(self, perm, **kwargs):
        super(TransposeLayer, self).__init__(**kwargs)
        self.perm = perm

    def call(self, inputs):
        return tf.transpose(inputs, perm=self.perm)

    def get_config(self):
        config = super(TransposeLayer, self).get_config()
        config.update({"perm": self.perm})
        return config

# 3. iTransformer Model
def build_transformer(input_shape, num_heads, ff_dim, num_layers, dropout, l2_lambda, noise_std):
    inputs = Input(shape=input_shape)  # input_shape = (window_size, feature_dim)
    x = inputs

    # Add Gaussian noise for data augmentation
    x = GaussianNoise(stddev=noise_std)(x, training=True)

    # Transpose to treat features as sequence: (batch, window_size, feature_dim) -> (batch, feature_dim, window_size)
    x = TransposeLayer(perm=[0, 2, 1])(x)

    # Positional Encoding for feature dimension
    def positional_encoding(length, d_model):
        pos = np.arange(length)[:, np.newaxis]
        i = np.arange(d_model)[np.newaxis, :]
        angle_rads = pos / np.power(10000, (2 * (i//2)) / np.float32(d_model))
        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])
        pos_encoding = angle_rads[np.newaxis, ...]
        return tf.cast(pos_encoding, dtype=tf.float32)

    pos_encoding = positional_encoding(input_shape[1], input_shape[0])  # Encode feature dimension
    x = x + pos_encoding

    # iTransformer Blocks
    for _ in range(num_layers):
        # Multi-Head Attention across features
        attention = MultiHeadAttention(num_heads=num_heads, key_dim=input_shape[0], dropout=dropout)(x, x)
        x = Add()([x, attention])
        x = LayerNormalization(epsilon=1e-6)(x)
        x = Dropout(dropout)(x)

        # Feed Forward Network
        ff = Dense(ff_dim, activation='relu', kernel_regularizer=l2(l2_lambda))(x)
        ff = Dense(input_shape[0], kernel_regularizer=l2(l2_lambda))(ff)  # Output dim matches window_size
        x = Add()([x, ff])
        x = LayerNormalization(epsilon=1e-6)(x)
        x = Dropout(dropout)(x)

    # Transpose back to original shape: (batch, feature_dim, window_size) -> (batch, window_size, feature_dim)
    x = TransposeLayer(perm=[0, 2, 1])(x)

    # Output Layers
    x = Flatten()(x)
    x = Dense(32, activation='relu', kernel_regularizer=l2(l2_lambda))(x)
    x = Dropout(dropout)(x)
    outputs = Dense(1)(x)

    return Model(inputs, outputs)

# 4. Optuna Objective Function
def objective(trial, X_train_seq, y_train_seq, X_test_seq, y_test_seq, train_date_seq, test_date_seq, y_scaler):
    # Define hyperparameter search space
    num_heads = trial.suggest_categorical('num_heads', [2, 4, 6, 8])
    ff_dim = trial.suggest_categorical('ff_dim', [32, 64, 128, 256])
    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])
    dropout = trial.suggest_categorical('dropout', [0.1, 0.2, 0.3, 0.4])
    l2_lambda = trial.suggest_categorical('l2_lambda', [0.01, 0.05, 0.1, 0.2])
    initial_lr = trial.suggest_categorical('initial_lr', [1e-4, 5e-4, 1e-3, 5e-3])
    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256, 512])
    noise_std = trial.suggest_categorical('noise_std', [0.05, 0.1, 0.2])

    WINDOW_SIZE = 90
    EPOCHS = 200
    WARMUP_EPOCHS = 20
    K_FOLDS = 5

    # K-Fold Cross Validation
    kfold = KFold(n_splits=K_FOLDS, shuffle=False)
    fold_val_losses = []

    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_seq, y_train_seq)):
        X_train_fold = X_train_seq[train_idx]
        y_train_fold = y_train_seq[train_idx]
        X_val_fold = X_train_seq[val_idx]
        y_val_fold = y_train_seq[val_idx]

        # Suppress logs during model compilation
        with suppress_output():
            model = build_transformer(
                input_shape=(WINDOW_SIZE, X_train_seq.shape[2]),
                num_heads=num_heads,
                ff_dim=ff_dim,
                num_layers=num_layers,
                dropout=dropout,
                l2_lambda=l2_lambda,
                noise_std=noise_std
            )
            model.compile(optimizer=Adam(learning_rate=initial_lr), loss='mse')

        early_stop = EarlyStopping(monitor='val_loss', patience=15, min_delta=0.0001, restore_best_weights=True)
        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0)
        checkpoint = ModelCheckpoint(f'best_model_fold_{fold}.keras', monitor='val_loss', save_best_only=True, save_weights_only=False, verbose=0)
        warmup_lr = WarmUpCosineDecay(
            initial_lr=initial_lr,
            warmup_epochs=WARMUP_EPOCHS,
            total_epochs=EPOCHS,
            steps_per_epoch=len(X_train_fold) // batch_size
        )

        # Training with suppressed logs
        with suppress_output():
            history = model.fit(
                X_train_fold, y_train_fold,
                epochs=EPOCHS,
                batch_size=batch_size,
                validation_data=(X_val_fold, y_val_fold),
                callbacks=[early_stop, reduce_lr, checkpoint, warmup_lr],
                verbose=0
            )

        val_loss = min(history.history['val_loss'])
        fold_val_losses.append(val_loss)

    # Return average validation loss across folds
    return np.mean(fold_val_losses)

# 5. Main Execution
def main():
    WINDOW_SIZE = 90
    EPOCHS = 200
    K_FOLDS = 5
    NUM_THREADS = 4  # Number of parallel threads for Optuna

    print("Starting shoreline prediction model with Optuna...")
    # Load and preprocess data
    with suppress_output():
        X_train, X_test, y_train, y_test, train_dates, test_dates, scaler, y_scaler = load_and_preprocess_data(
            '/home/ubuntu/DeepLearning/otama_shore_wave.csv')
        print("Data loaded successfully.")  # Debug print (suppressed)

        X_train_seq, y_train_seq, train_date_seq = create_sequences(X_train, y_train, train_dates, WINDOW_SIZE)
        X_test_seq, y_test_seq, test_date_seq = create_sequences(X_test, y_test, test_dates, WINDOW_SIZE)

    # DB for multi-threading
    storage_name = "sqlite:///AMG_shoreline.db" 
    study_name = "multithread_study"

    study = optuna.create_study(
        storage=storage_name, 
        study_name=study_name, 
        load_if_exists=True,
        direction='minimize'
    )

    # Run Optuna optimization
    #study = optuna.create_study(direction='minimize')
    study.optimize(
        lambda trial: objective(trial, X_train_seq, y_train_seq, X_test_seq, y_test_seq, train_date_seq, test_date_seq, y_scaler),
        n_trials=50,
        n_jobs=NUM_THREADS
    )

    # Print best hyperparameters
    print("\nBest Hyperparameters:")
    print(study.best_params)
    print(f"Best Average Validation Loss: {study.best_value:.4f}")

    # Train final model with best hyperparameters
    best_params = study.best_params
    with suppress_output():
        model = build_transformer(
            input_shape=(WINDOW_SIZE, X_train_seq.shape[2]),
            num_heads=best_params['num_heads'],
            ff_dim=best_params['ff_dim'],
            num_layers=best_params['num_layers'],
            dropout=best_params['dropout'],
            l2_lambda=best_params['l2_lambda'],
            noise_std=best_params['noise_std']
        )
        model.compile(optimizer=Adam(learning_rate=best_params['initial_lr']), loss='mse')

    early_stop = EarlyStopping(monitor='val_loss', patience=15, min_delta=0.0001, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)
    checkpoint = ModelCheckpoint('best_model_optuna.keras', monitor='val_loss', save_best_only=True, save_weights_only=False, verbose=0)
    warmup_lr = WarmUpCosineDecay(
        initial_lr=best_params['initial_lr'],
        warmup_epochs=20,
        total_epochs=EPOCHS,
        steps_per_epoch=len(X_train_seq) // best_params['batch_size']
    )

    # Train on full training data
    history = model.fit(
        X_train_seq, y_train_seq,
        epochs=EPOCHS,
        batch_size=best_params['batch_size'],
        validation_split=0.2,
        callbacks=[early_stop, reduce_lr, checkpoint, warmup_lr],
        verbose=2
    )

    # Load the best saved model
    model = tf.keras.models.load_model('best_model_optuna.keras', custom_objects={'TransposeLayer': TransposeLayer})

    # Suppress logs during prediction
    with suppress_output():
        train_pred = model.predict(X_train_seq)
        test_pred = model.predict(X_test_seq)

        # Inverse transform predictions
        train_pred = y_scaler.inverse_transform(train_pred).flatten()
        test_pred = y_scaler.inverse_transform(test_pred).flatten()

        y_train_seq = y_scaler.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()
        y_test_seq = y_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()

    # Print metrics
    print("\nEvaluation Metrics (Test):")
    print(f"Test MAE: {mean_absolute_error(y_test_seq, test_pred):.2f}")
    print(f"Test RMSE: {np.sqrt(mean_squared_error(y_test_seq, test_pred)):.2f}")
    print(f"Test R²: {r2_score(y_test_seq, test_pred):.2f}")

    # Plot training loss
    plt.figure(figsize=(10, 5))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss (Optuna)')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.savefig('training_loss_optuna.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Plot predictions
    plt.figure(figsize=(14, 6))
    plt.plot(train_date_seq, y_train_seq, label='Actual (Train)', color='blue', alpha=0.7)
    plt.plot(train_date_seq, train_pred, label='Predicted (Train)', color='cyan', linestyle='--')
    plt.plot(test_date_seq, y_test_seq, label='Actual (Test)', color='green', alpha=0.7)
    plt.plot(test_date_seq, test_pred, label='Predicted (Test)', color='red', linestyle='--')

    plt.title('Shoreline Position Prediction (1999-2025) - Optuna')
    plt.xlabel('Year')
    plt.ylabel('Shoreline Position')
    plt.legend()
    plt.grid(True)
    plt.gca().xaxis.set_major_locator(mdates.YearLocator(4))
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
    plt.gcf().autofmt_xdate()
    plt.savefig('Otama_iT_Kfold_Optuna .png', dpi=300, bbox_inches='tight')
    plt.show()

    # Save results with three decimal places
    results = pd.DataFrame({
        'Date': np.concatenate([train_date_seq, test_date_seq]),
        'Actual': np.concatenate([y_train_seq, y_test_seq]),
        'Predicted': np.concatenate([train_pred, test_pred])
    })
    results.to_csv('Otama_iT_Kfold_Optuna.csv', index=False, float_format='%.3f')

if __name__ == "__main__":
    main()
